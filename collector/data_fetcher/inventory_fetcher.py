"""
åº“å­˜æŠ“å–å™¨ - ä» COMEX / SHFE / LBMA è·å–åº“å­˜æ•°æ®ã€‚

æ•°æ®æºï¼š
- COMEX: CME å®˜æ–¹å…¬å¼€ XLS æ–‡ä»¶ï¼ˆGold_Stocks.xls / Silver_stocks.xlsï¼‰
- SHFE:  ä¸ŠæœŸæ‰€å®˜æ–¹ JSON API (pm{date}.dat)
- LBMA:  ä¼¦æ•¦é‡‘åº“æœˆåº¦ XLSX æ–‡ä»¶
"""
import io
import json
import logging
import re
import time
from datetime import datetime, timedelta
from typing import Any

import pandas as pd
import requests
from scrapling import Fetcher, StealthyFetcher

from collector.database import DatabaseManager
from collector.settings import PROXIES, USE_PROXY

logger = logging.getLogger(__name__)

# ç›å¸ â†’ å¨ æ¢ç®—å› å­
OUNCE_TO_TON = 32150.7466

# CME å®˜æ–¹åº“å­˜æŠ¥å‘Š URL
COMEX_URLS: dict[str, str] = {
    "gold": "https://www.cmegroup.com/delivery_reports/Gold_Stocks.xls",
    "silver": "https://www.cmegroup.com/delivery_reports/Silver_stocks.xls",
}

# å¸¸ç”¨ User-Agent
_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)


class InventoryFetcher:
    """ä¸‰å¤§å¸‚åœºåº“å­˜æ•°æ®æŠ“å–å™¨ï¼ˆCOMEX / SHFE / LBMAï¼‰ã€‚"""

    def __init__(self, db: DatabaseManager) -> None:
        self.db = db
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": _UA})
        if USE_PROXY:
            self.session.proxies.update(PROXIES)

    # ============================================================
    # COMEX â€”â€” CME å®˜æ–¹ XLS ç›´æ¥ä¸‹è½½è§£æ
    # ============================================================

    def fetch_comex(self, metal: str = "gold") -> list[dict[str, Any]]:
        """
        ä¸‹è½½ CME å®˜æ–¹åº“å­˜ XLS æ–‡ä»¶ï¼Œè§£æå„ä»“åº“çš„ Registered/Eligible/Totalã€‚

        Args:
            metal: 'gold' æˆ– 'silver'ã€‚

        Returns:
            åº“å­˜è®°å½•åˆ—è¡¨ã€‚
        """
        url = COMEX_URLS[metal]
        records: list[dict[str, Any]] = []
        try:
            logger.info("COMEX %s: ä¸‹è½½ %s", metal, url)
            resp = self.session.get(url, timeout=30)
            resp.raise_for_status()

            # è¯»å– XLS
            df = pd.read_excel(
                io.BytesIO(resp.content),
                header=None,
                engine="xlrd",  # .xls æ ¼å¼éœ€è¦ xlrd
            )

            # 1) æå– Report Date
            report_date = self._extract_report_date(df, metal)

            # 2) è§£æä»“åº“æ•°æ®
            records = self._parse_comex_xls(df, metal, report_date)

            logger.info(
                "COMEX %s: è·å– %d æ¡åº“å­˜è®°å½• (æ—¥æœŸ: %s)",
                metal, len(records), report_date,
            )
        except Exception:
            logger.exception("COMEX %s åº“å­˜æŠ“å–å¤±è´¥", metal)
        return records

    def _extract_report_date(self, df: pd.DataFrame, metal: str) -> str:
        """ä» XLS å‰å‡ è¡Œæå– Report Dateã€‚"""
        for idx in range(min(10, len(df))):
            cell = str(df.iloc[idx, 0]) if pd.notna(df.iloc[idx, 0]) else ""
            # åŒ¹é… "Report Date: 2/6/2026" æˆ–ç±»ä¼¼æ ¼å¼
            m = re.search(r"Report\s*Date[:\s]*(\d{1,2}/\d{1,2}/\d{4})", cell)
            if m:
                try:
                    return datetime.strptime(m.group(1), "%m/%d/%Y").strftime("%Y-%m-%d")
                except ValueError:
                    pass
            # ä¹ŸåŒ¹é… "Activity Date"
            m = re.search(r"Activity\s*Date[:\s]*(\d{1,2}/\d{1,2}/\d{4})", cell)
            if m:
                try:
                    return datetime.strptime(m.group(1), "%m/%d/%Y").strftime("%Y-%m-%d")
                except ValueError:
                    pass
        # å…œåº•ç”¨ä»Šå¤©
        logger.warning("COMEX %s: æœªæ‰¾åˆ° Report Date, ä½¿ç”¨ä»Šå¤©", metal)
        return datetime.now().strftime("%Y-%m-%d")

    def _parse_comex_xls(
        self, df: pd.DataFrame, metal: str, report_date: str
    ) -> list[dict[str, Any]]:
        """
        è§£æ CME XLS ä»“åº“ç»“æ„ï¼š
        - ä»“åº“åè¡Œï¼šç¬¬ä¸€åˆ—æœ‰å€¼ï¼Œå…¶ä½™åˆ—å¤§å¤šä¸º NaN
        - ç„¶åæ˜¯ Registered / Eligible / Total ç­‰è¡Œ
        """
        records: list[dict[str, Any]] = []
        current_warehouse = None

        # æ‰¾åˆ°è¡¨å¤´è¡Œï¼ˆåŒ…å« DEPOSITORY æˆ– PREV TOTAL çš„è¡Œï¼‰
        header_idx = None
        for idx in range(len(df)):
            cell0 = str(df.iloc[idx, 0]).strip().upper() if pd.notna(df.iloc[idx, 0]) else ""
            if "DEPOSITORY" in cell0 or "PREV" in cell0:
                header_idx = idx
                break

        if header_idx is None:
            logger.warning("COMEX %s: æœªæ‰¾åˆ°è¡¨å¤´è¡Œ", metal)
            return records

        warehouse_data: dict[str, dict[str, float]] = {}
        total_data: dict[str, float] = {}

        for idx in range(header_idx + 1, len(df)):
            row = df.iloc[idx]
            cell0 = str(row.iloc[0]).strip() if pd.notna(row.iloc[0]) else ""

            if not cell0:
                continue

            # åˆ¤æ–­æ˜¯å¦æ˜¯ä»“åº“åç§°è¡Œï¼ˆç¬¬ä¸€åˆ—æœ‰å€¼ï¼Œåé¢åˆ—å¤§éƒ¨åˆ† NaNï¼‰
            non_null_count = row.notna().sum()
            if non_null_count <= 2 and cell0 and not any(
                kw in cell0.upper() for kw in
                ["REGISTERED", "ELIGIBLE", "PLEDGED", "TOTAL", "GRAND", "---"]
            ):
                current_warehouse = cell0
                if current_warehouse not in warehouse_data:
                    warehouse_data[current_warehouse] = {}
                continue

            # è·³è¿‡åˆ†éš”çº¿
            if "---" in cell0 or cell0.startswith("="):
                continue

            # æå– TOTAL TODAY åˆ—å€¼ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€ä¸ªæœ‰æ•°æ®çš„åˆ—ï¼‰
            cell0_upper = cell0.upper()
            today_val = self._get_today_value(row)

            if current_warehouse and today_val is not None:
                if "REGISTERED" in cell0_upper:
                    warehouse_data[current_warehouse]["registered"] = today_val
                elif "ELIGIBLE" in cell0_upper:
                    warehouse_data[current_warehouse]["eligible"] = today_val

            # æ£€æµ‹ GRAND TOTAL è¡Œ
            if "GRAND" in cell0_upper and "TOTAL" in cell0_upper:
                # å…¨å±€æ±‡æ€»è¡Œ
                break

            # æ£€æµ‹ TOTAL è¡Œ (ä»“åº“çº§åˆ«æ±‡æ€»)
            if "TOTAL" in cell0_upper and current_warehouse:
                if today_val is not None:
                    warehouse_data[current_warehouse]["total"] = today_val

        # ç”Ÿæˆè®°å½•
        for wh_name, data in warehouse_data.items():
            if not data:
                continue
            registered = data.get("registered", 0)
            eligible = data.get("eligible", 0)
            total = data.get("total", registered + eligible)

            if total > 0:
                # æ³¨å†Œåº“å­˜
                records.append({
                    "date": report_date,
                    "exchange": "COMEX",
                    "metal": metal,
                    "category": "registered",
                    "warehouse": wh_name,
                    "inventory": registered,
                    "unit": "oz",
                    "source": "cme_xls",
                })
                # åˆæ ¼åº“å­˜
                records.append({
                    "date": report_date,
                    "exchange": "COMEX",
                    "metal": metal,
                    "category": "eligible",
                    "warehouse": wh_name,
                    "inventory": eligible,
                    "unit": "oz",
                    "source": "cme_xls",
                })
                # æ€»é‡ï¼ˆå¨ï¼‰
                records.append({
                    "date": report_date,
                    "exchange": "COMEX",
                    "metal": metal,
                    "category": "total",
                    "warehouse": wh_name,
                    "inventory": round(total / OUNCE_TO_TON, 4),
                    "unit": "ton",
                    "source": "cme_xls",
                })

        # ç”Ÿæˆ COMEX å…¨å±€æ±‡æ€»è¡Œï¼ˆwarehouse=''ï¼‰
        if warehouse_data:
            total_registered = sum(d.get("registered", 0) for d in warehouse_data.values() if d)
            total_eligible = sum(d.get("eligible", 0) for d in warehouse_data.values() if d)
            grand_total = total_registered + total_eligible
            for cat, val, unit in [
                ("registered", total_registered, "oz"),
                ("eligible", total_eligible, "oz"),
                ("total", round(grand_total / OUNCE_TO_TON, 4), "ton"),
            ]:
                records.append({
                    "date": report_date,
                    "exchange": "COMEX",
                    "metal": metal,
                    "category": cat,
                    "warehouse": "",
                    "inventory": val,
                    "unit": unit,
                    "source": "cme_xls",
                })

        return records

    @staticmethod
    def _get_today_value(row: pd.Series) -> float | None:
        """ä»è¡Œä¸­æå– TOTAL TODAY åˆ—å€¼ï¼ˆé€šå¸¸å€’æ•°å‡ åˆ—ä¸­æœ€åä¸€ä¸ªæœ‰æ•ˆæ•°å€¼ï¼‰ã€‚"""
        # ä½¿ç”¨ pd.to_numeric ç»Ÿä¸€å¤„ç†ï¼šdatetime/å­—ç¬¦ä¸²/ä¹±ç  â†’ NaN
        numeric_row = pd.to_numeric(row.iloc[1:], errors="coerce")
        valid = numeric_row.dropna()
        if valid.empty:
            return None
        # è¿”å›æœ€åä¸€ä¸ªæœ‰æ•ˆæ•°å€¼ï¼ˆTOTAL TODAY åˆ—ï¼‰
        return float(valid.iloc[-1])

    # ============================================================
    # SHFE â€”â€” ä¸ŠæœŸæ‰€ JSON API
    # ============================================================

    def fetch_shfe(self, date: str | None = None) -> list[dict[str, Any]]:
        """
        ä»ä¸ŠæœŸæ‰€ JSON API è·å–ä»“å•æ•°æ®ã€‚
        å¢åŠ å›æº¯æœºåˆ¶ï¼Œå¦‚æœå½“å¤©æœªå‘å¸ƒï¼ˆå¦‚å‘¨æœ«æˆ–èŠ‚å‡æ—¥è¿”å› 404ï¼‰ï¼Œåˆ™å‘å‰å°è¯•æœ€å¤š 7 å¤©ã€‚
        ä½¿ç”¨ Scrapling ä¼ªè£…æµè§ˆå™¨æŒ‡çº¹ä»¥ç»•è¿‡åçˆ¬è™«ã€‚
        åŠ å…¥é’ˆå¯¹ç‰¹å®šæ¥å£è¿ç»­ 404 çš„ç†”æ–­ä¸æŠ¥è­¦é‚®ä»¶æœºåˆ¶ã€‚
        """
        records: list[dict[str, Any]] = []
            
        target_dt = datetime.strptime(date, "%Y%m%d") if date else datetime.now()
        
        continuous_404_count = 0
        
        # å°è¯•æœ€å¤š 7 å¤© (æ¶µç›–é•¿å‡)
        for offset in range(7):
            curr_dt = target_dt - timedelta(days=offset)
            date_str = curr_dt.strftime("%Y%m%d")
            date_fmt = curr_dt.strftime("%Y-%m-%d")
            # ä½¿ç”¨æµè§ˆå™¨å¼•æ“å¤„ç†æ½œä¼çš„é‡å®šå‘å’Œå®‰å…¨æ£€æŸ¥
            # æ ¹æ®æœ€æ–°è°ƒç ”ï¼ŒSHFE æ•°æ®è·¯å¾„å·²å˜æ›´ä¸º /data/tradedata/future/dailydata/
            url = f"https://www.shfe.com.cn/data/tradedata/future/dailydata/pm{date_str}.dat"
            logger.info("å°è¯•ä» SHFE æŠ“å–ä»“å•æ•°æ® (Scrapling): %s", url)
            
            try:
                # ä½¿ç”¨ scrapling StealthyFetcher è‡ªåŠ¨å¤„ç†æŒ‡çº¹å’Œç»•è¿‡ (å…³é—­ headless æé«˜éšè”½æ€§ï¼Œå€ŸåŠ© xvfb)
                resp = StealthyFetcher.fetch(url, timeout=15000, headless=False)
                
                if resp.status == 404:
                    logger.debug("SHFE pm%s.dat æŠ¥ 404 (æ— æ•°æ®/éäº¤æ˜“æ—¥)ï¼Œå°è¯•å›é€€...", date_str)
                    continuous_404_count += 1
                    continue
                else:
                    # åªè¦æœ‰é 404 è¿”å›ï¼Œæ‰“ç ´è¿ç»­ 404 è®¡æ•°
                    continuous_404_count = 0
                
                if resp.status != 200:
                    continue
                
                try:
                    data = json.loads(resp.text)
                except Exception:
                    logger.error("SHFE JSON è§£æå¤±è´¥: %s", resp.text[:100])
                    continue

                for item in data.get("o_cursor", []):
                    var_name = str(item.get("VARNAME", "")).strip().upper()
                    if "AU" in var_name:
                        metal_val = "gold"
                    elif "AG" in var_name:
                        metal_val = "silver"
                    else:
                        continue

                    warehouse = str(item.get("REGNAME", "")).strip()
                    if not warehouse or warehouse == "nan":
                        warehouse = str(item.get("WHABBRNAME", "")).strip()

                    weight_str = str(item.get("WRTWGHTS", "0")).strip()
                    try:
                        weight = float(weight_str.replace(",", ""))
                    except (ValueError, TypeError):
                        weight = 0.0

                    unit_val = str(item.get("WGHTUNIT", "åƒå…‹")).strip()

                    if weight > 0:
                        records.append({
                            "date": date_fmt,
                            "exchange": "SHFE",
                            "metal": metal_val,
                            "category": "warehouse",
                            "warehouse": warehouse,
                            "inventory": weight,
                            "unit": unit_val,
                            "source": "shfe_json_scrapling",
                        })

                if records:
                    logger.info("SHFE è·å– %d æ¡ä»“å•è®°å½• (å›æº¯ %d å¤©ï¼Œæ—¥æœŸ: %s)", len(records), offset, date_fmt)
                    return records
                else:
                    logger.warning("SHFE %s æ•°æ®è§£æä¸ºç©ºï¼Œç»§ç»­å°è¯•...", date_fmt)

            except Exception as e:
                logger.warning("SHFE %s ä»“å•æŠ“å–å¼‚å¸¸: %s", date_fmt, e)
                
            time.sleep(0.5)
            
        logger.warning("SHFE å›æº¯ 7 å¤©ä»æœªè·å–åˆ°ä»“å•æ•°æ®")
        
        # è§¦å‘ç‰¹çº§ç†”æ–­è­¦æŠ¥é‚®ä»¶
        if continuous_404_count >= 3:
            logger.error("SHFE æ¥å£è¿ç»­ 3 å¤©åŠä»¥ä¸Šè¿”å› 404ï¼Œå¯èƒ½è·¯å¾„å‘ç”Ÿåç§»ï¼Œå‘é€æŠ¥è­¦é‚®ä»¶...")
            try:
                from collector.mailer import EmailSender
                sender = EmailSender()
                msg = f"<h3>ğŸš¨ SHFE æ¥å£è®¿é—®å¼‚å¸¸ç†”æ–­è­¦æŠ¥ ğŸš¨</h3><p>ç³»ç»Ÿè¿ç»­ {continuous_404_count} æ¬¡å°è¯•è®¿é—® SHFE ä»“å•æ¥å£ <strong>/data/tradedata/future/dailydata/</strong> å‡è¿”å› 404ã€‚</p><p>è¯·ç›¸å…³è¿ç»´äººå‘˜ç«‹åˆ»æ£€æŸ¥å¹¶é‡å†™æ¥å£æå–è§„åˆ™ï¼</p>"
                sender.send_email(msg, datetime.now().strftime("%Y-%m-%d") + " (SHFE å‘Šè­¦)", None)
            except Exception as e:
                logger.error("å‘é€ SHFE ç†”æ–­æŠ¥è­¦é‚®ä»¶å¤±è´¥: %s", e)
                
        return records

    # ============================================================
    # LBMA â€”â€” ä¼¦æ•¦é‡‘åº“æœˆåº¦ XLSX
    # ============================================================

    def fetch_lbma(self, year: int | None = None, month: int | None = None) -> list[dict[str, Any]]:
        """
        ä¸‹è½½ LBMA æœˆåº¦ä¼¦æ•¦é‡‘åº“æ•°æ®ã€‚

        æ–‡ä»¶ URL æ ¼å¼:
        https://cdn.lbma.org.uk/downloads/LBMA-London-Vault-Holdings-Data-{Month}-{Year}.xlsx

        Args:
            year: å¹´ä»½ï¼Œé»˜è®¤å½“å‰å¹´ã€‚
            month: æœˆä»½ï¼Œé»˜è®¤ä¸Šä¸ªæœˆï¼ˆLBMA æ•°æ®æœ‰ 1 ä¸ªæœˆå»¶è¿Ÿï¼‰ã€‚

        Returns:
            åº“å­˜è®°å½•åˆ—è¡¨ã€‚
        """
        records: list[dict[str, Any]] = []
        now = datetime.now()

        if year is None or month is None:
            # LBMA æ•°æ®é€šå¸¸æœ‰ 1 ä¸ªæœˆå»¶è¿Ÿï¼Œå–ä¸Šä¸ªæœˆ
            if now.month == 1:
                year = now.year - 1
                month = 12
            else:
                year = now.year
                month = now.month - 1

        month_names = [
            "", "January", "February", "March", "April", "May", "June",
            "July", "August", "September", "October", "November", "December",
        ]
        month_name = month_names[month]

        xlsx_url = (
            f"https://cdn.lbma.org.uk/downloads/"
            f"LBMA-London-Vault-Holdings-Data-{month_name}-{year}.xlsx"
        )
        
        try:
            from curl_cffi import requests as cffi_requests
            
            # ä½¿ç”¨ Scrapling è·å– Cloudflare cookies (headed æ¨¡å¼ä»¥æé«˜éšè”½æ€§é€šè¿‡æŒ‘æˆ˜)
            auth_url = "https://www.lbma.org.uk/prices-and-data/london-vault-data"
            logger.info("LBMA: ä½¿ç”¨ Scrapling è·å– Cloudflare é‰´æƒä¿¡æ¯...")
            fetcher_resp = StealthyFetcher.fetch(auth_url, headless=False, solve_cloudflare=True, wait=3000)
            
            cookies_tuple = getattr(fetcher_resp, "cookies", ())
            cookies_dict = {c['name']: c['value'] for c in cookies_tuple if isinstance(c, dict) and 'name' in c and 'value' in c}
            
            user_agent = _UA
            if hasattr(fetcher_resp, "request") and hasattr(fetcher_resp.request, "headers"):
                user_agent = fetcher_resp.request.headers.get("User-Agent", _UA)
                
            logger.info("LBMA: é‰´æƒè·å–æˆåŠŸï¼Œå‡†å¤‡ä¸‹è½½ XLSX: %s", xlsx_url)
            
            headers = {
                "User-Agent": user_agent,
                "Referer": "https://www.lbma.org.uk/",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
            }
            
            # ä½¿ç”¨ curl_cffi ä¸‹è½½äºŒè¿›åˆ¶æ–‡ä»¶ (å¤ç”¨ Cookie å’Œ UA)
            xlsx_resp = cffi_requests.get(
                xlsx_url,
                cookies=cookies_dict,
                headers=headers,
                impersonate="chrome",
                timeout=30
            )
            
            if xlsx_resp.status_code == 404:
                logger.warning("LBMA %d-%02d æ•°æ®ä¸å­˜åœ¨ï¼ˆå¯èƒ½å°šæœªå‘å¸ƒï¼‰", year, month)
                return records
                
            xlsx_resp.raise_for_status()

            df = pd.read_excel(io.BytesIO(xlsx_resp.content), header=None, engine="openpyxl")
            logger.info("LBMA åŸå§‹è¡Œæ•°: %d, åˆ—æ•°: %d", len(df), len(df.columns))

            # è§£æï¼šä»ç¬¬ 3 è¡Œï¼ˆç´¢å¼• 2ï¼‰å¼€å§‹ï¼Œæ‰¾ YYYY-MM æ ¼å¼çš„æ—¥æœŸåˆ—
            for idx in range(2, len(df)):
                month_end = str(df.iloc[idx, 0]).strip()
                if not re.match(r"\d{4}-\d{2}", month_end):
                    continue

                # Gold = åˆ— 1, Silver = åˆ— 2ï¼ˆåƒç›å¸ï¼‰
                gold_koz = self._safe_float_val(df.iloc[idx, 1])
                silver_koz = self._safe_float_val(df.iloc[idx, 2])

                if gold_koz is not None and gold_koz > 0:
                    gold_ton = (gold_koz * 1000) / OUNCE_TO_TON
                    records.append({
                        "date": month_end,
                        "exchange": "LBMA",
                        "metal": "gold",
                        "category": "vault_total",
                        "warehouse": "London Vaults",
                        "inventory": round(gold_ton, 2),
                        "unit": "ton",
                        "source": "lbma_xlsx",
                    })
                if silver_koz is not None and silver_koz > 0:
                    silver_ton = (silver_koz * 1000) / OUNCE_TO_TON
                    records.append({
                        "date": month_end,
                        "exchange": "LBMA",
                        "metal": "silver",
                        "category": "vault_total",
                        "warehouse": "London Vaults",
                        "inventory": round(silver_ton, 2),
                        "unit": "ton",
                        "source": "lbma_xlsx",
                    })

            # åªå–æœ€æ–° 2 æ¡æœˆä»½æ•°æ®ï¼ˆé¿å…å†™å…¥è¿‡å¤šå†å²ï¼‰
            if len(records) > 4:
                records = records[:4]

            logger.info("LBMA è·å– %d æ¡é‡‘åº“è®°å½•", len(records))

        except ImportError:
            logger.error("ç¼ºå°‘ curl_cffi åº“ï¼Œæ— æ³•æŠ“å– LBMA")
        except Exception:
            logger.exception("LBMA é‡‘åº“æ•°æ®æŠ“å–æˆ–è§£æå¤±è´¥")
        return records

    # ============================================================
    # SGE â€”â€” ä¸Šæµ·é‡‘äº¤æ‰€ PDF
    # ============================================================

    def fetch_sge_pdf(self, date: str | None = None) -> list[dict[str, Any]]:
        """
        ä»ä¸Šæµ·é‡‘äº¤æ‰€ JSON API ç›´æ¥æå– PDF é“¾æ¥ï¼Œç»•è¿‡ Vue åŠ¨æ€æ¸²æŸ“é—®é¢˜ã€‚
        """
        records: list[dict[str, Any]] = []

        records: list[dict[str, Any]] = []
        fetcher = Fetcher()
        import os
        import tempfile
            
        # SGE æ–‡ç« åˆ—è¡¨ API (menuId=1738 ä¸ºæ¯æ—¥è¡Œæƒ…)
        api_url = "https://www.sge.com.cn/public/front/findArticleExtList?pageNo=1&pageSize=15&menuId=1738"
        headers = {
            "User-Agent": _UA,
            "Accept": "application/json, text/plain, */*",
            "Referer": "https://www.sge.com.cn/sjzx/mrhq"
        }
        
        try:
            logger.debug("è¯·æ±‚ SGE JSON API: %s", api_url)
            resp = StealthyFetcher.fetch(
                api_url, 
                timeout=15000,
                headless=False
            )
            if resp.status != 200:
                logger.warning("SGE JSON API è¿”å›é”™è¯¯çŠ¶æ€ç : %s", resp.status)
                return records
                
            try:
                data = json.loads(resp.text)
            except Exception:
                logger.error("SGE JSON è§£æå¤±è´¥: %s", resp.text[:100])
                return records
            
            articles = data.get("list", [])
            if not articles:
                logger.warning("SGE JSON API æœªè¿”å›æ–‡ç« åˆ—è¡¨")
                return records

            # å¯»æ‰¾æœ€æ–°çš„â€œäº¤å‰²â€æˆ–â€œäº¤æ”¶â€ç±»æ–‡ç« 
            target_article = None
            for item in articles:
                title = item.get("title", "")
                if "äº¤å‰²" in title or "äº¤æ”¶" in title or "è¡Œæƒ…" in title:
                    target_article = item
                    break
            
            if not target_article:
                logger.warning("SGE JSON åˆ—è¡¨å†…æœªæ‰¾åˆ°äº¤å‰²ç›¸å…³æŠ¥å‘Š")
                return records

            pdf_path_relative = target_article.get("fileUrl")
            if not pdf_path_relative:
                logger.warning("SGE æ–‡ç«  '%s' ç¼ºå°‘ PDF é“¾æ¥", target_article.get("title"))
                return records

            pdf_url = "https://www.sge.com.cn" + pdf_path_relative
            publish_date = target_article.get("publishDate", "").split(" ")[0]
            
            logger.info("å‡†å¤‡ä¸‹è½½ SGE PDF: %s (å‘å¸ƒæ—¥æœŸ: %s)", pdf_url, publish_date)
            
            pdf_resp = StealthyFetcher.fetch(
                pdf_url, 
                timeout=30000,
                headless=False
            )
            if pdf_resp.status != 200:
                logger.error("SGE PDF ä¸‹è½½å¤±è´¥: %s", pdf_resp.status)
                return records
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(pdf_resp.body)
                tmp_path = tmp.name

            records = self._parse_sge_pdf(tmp_path, date_str=publish_date)
            os.remove(tmp_path)

        except Exception:
            logger.exception("SGE JSON æ•°æ®æŠ“å–æˆ– PDF è§£æå¤±è´¥")

        return records

    @staticmethod
    def _parse_sge_pdf(pdf_path: str, date_str: str) -> list[dict[str, Any]]:
        """
        è§£æ SGE PDF æ–‡ä»¶ï¼Œæå–äº¤å‰²æ•°æ®ã€‚
        """
        records: list[dict[str, Any]] = []
        gold_vol = 0.0
        silver_vol = 0.0

        try:
            import pdfplumber
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    tables = page.extract_tables()
                    for table in tables:
                        # åŠ¨æ€è¡¨å¤´å¯»æ‰¾
                        delivery_col_idx = -1
                        for row_idx, row in enumerate(table):
                            if not row: continue
                            row_str_list = [str(x).replace('\n', '') for x in row if x]
                            row_text = "".join(row_str_list).upper()
                            
                            # æ¢æµ‹è¡¨å¤´ä¸­çš„â€œäº¤æ”¶â€æˆ–â€œäº¤å‰²â€å­—çœ¼
                            if "äº¤æ”¶" in row_text or "äº¤å‰²" in row_text:
                                for col_idx, cell_text in enumerate(row):
                                    if cell_text and re.search(r"(äº¤æ”¶|äº¤å‰²)", str(cell_text)):
                                        delivery_col_idx = col_idx
                                        break
                                        
                            # å¦‚æœæ‰¾åˆ°äº†è¡¨å¤´ï¼Œæˆ–è€…æœ¬èº«å·²ç»æ˜¯æ•°æ®è¡Œ
                            if "AU" in row_text or "é‡‘" in row_text or "AG" in row_text or "é“¶" in row_text:
                                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„è¡¨å¤´åˆ—ï¼Œå°è¯•ç”¨å¯å‘å¼çš„æ­£åˆ™ (é€šå¸¸äº¤å‰²é‡åœ¨å€’æ•°ç¬¬1/2åˆ—)
                                target_val = 0.0
                                
                                if delivery_col_idx != -1 and delivery_col_idx < len(row):
                                    # æŒ‰ç²¾ç¡®åˆ—ç´¢å¼•å–
                                    cell_str = str(row[delivery_col_idx]).replace(",", "").strip()
                                    num_match = re.search(r"[\d\.]+", cell_str)
                                    if num_match:
                                        target_val = float(num_match.group(0))
                                else:
                                    # å›é€€åˆ°æå–æ‰€æœ‰æ•°å­—
                                    nums = []
                                    for cell in row:
                                        if cell:
                                            num_match = re.search(r"[\d\.]+", str(cell).replace(",", ""))
                                            if num_match:
                                                try:
                                                    nums.append(float(num_match.group(0)))
                                                except ValueError:
                                                    pass
                                    if nums:
                                        # ä¿å®ˆæå–ï¼šå¦‚æœæå–ä¸åˆ°è¡¨å¤´ï¼Œæš‚ä¼°åœ¨æœ€åä¸€ä¸¤ä¸ªæ•°å­—é‡Œ
                                        # ä¼˜å…ˆå–æœ€åä¸€ä¸ªæ•°å­—ï¼Œå› ä¸ºäº¤å‰²é‡é€šå¸¸åœ¨è¡¨æ ¼çš„å³ä¾§
                                        target_val = nums[-1] if nums else 0.0

                                if target_val > 0:
                                    if "AU" in row_text or "é‡‘" in row_text:
                                        gold_vol += target_val
                                    elif "AG" in row_text or "é“¶" in row_text:
                                        silver_vol += target_val

            if gold_vol > 0:
                records.append({
                    "date": date_str,
                    "exchange": "SGE",
                    "metal": "gold",
                    "category": "delivery_volume",
                    "warehouse": "SGE Main",
                    "inventory": round(gold_vol / 1000, 4),
                    "unit": "ton",
                    "source": "sge_pdf",
                })
            if silver_vol > 0:
                records.append({
                    "date": date_str,
                    "exchange": "SGE",
                    "metal": "silver",
                    "category": "delivery_volume",
                    "warehouse": "SGE Main",
                    "inventory": round(silver_vol / 1000, 4),
                    "unit": "ton",
                    "source": "sge_pdf",
                })

            if records:
                logger.info("SGE è·å– %d æ¡ PDF è§£æè®°å½• (æ—¥æœŸ: %s)", len(records), date_str)

        except Exception as e:
            logger.warning("SGE PDF è§£æå¼‚å¸¸: %s", e)
        
        return records

    @staticmethod
    def _safe_float_val(val: Any) -> float | None:
        """å®‰å…¨è½¬æ¢æµ®ç‚¹æ•°ã€‚"""
        if pd.isna(val):
            return None
        try:
            return float(str(val).replace(",", "").strip())
        except (ValueError, TypeError):
            return None

    # ============================================================
    # æ±‡æ€»å…¥åº“
    # ============================================================

    def update_daily(self) -> int:
        """
        æ‰§è¡Œæ—¥åº¦åº“å­˜æ•°æ®æŠ“å–å¹¶å†™å…¥æ•°æ®åº“ã€‚

        Returns:
            æ€»æ’å…¥è®°å½•æ•°ã€‚
        """
        all_records: list[dict[str, Any]] = []

        # 1. COMEX é‡‘/é“¶
        for metal in ("gold", "silver"):
            comex = self.fetch_comex(metal)
            all_records.extend(comex)
            if not comex:
                logger.warning("COMEX %s æ— æ•°æ®ï¼ˆå¯èƒ½ CME è®¿é—®å—é™ï¼‰", metal)
            time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿ

        # 2. SHFE ä»“å•
        shfe = self.fetch_shfe()
        all_records.extend(shfe)

        # 3. LBMA æœˆåº¦ï¼ˆæ¯å¤©éƒ½å°è¯•ï¼Œé æ•°æ®åº“ INSERT OR REPLACE å»é‡ï¼‰
        lbma = self.fetch_lbma()
        all_records.extend(lbma)

        # 4. SGE PDF è§£æ
        sge = self.fetch_sge_pdf()
        all_records.extend(sge)

        if all_records:
            return self.db.insert_batch("inventory_daily", all_records)
        logger.warning("æ— åº“å­˜æ•°æ®å¯å†™å…¥")
        return 0


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    db = DatabaseManager()
    fetcher = InventoryFetcher(db)
    count = fetcher.update_daily()
    print(f"âœ… åº“å­˜æ•°æ®æŠ“å–å®Œæˆï¼Œå…± {count} æ¡è®°å½•")
